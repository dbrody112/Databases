{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findEndIndex(string):\n",
    "    for i in range(len(string)):\n",
    "        if(string[i] == \"        <br/>\"or string[i] == \"        ' <br/> '\"):\n",
    "            return i;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPages(url = \"https://www.hrc.org/resources/healthcare-facilities/\",string_input=\"New Jersey\"):\n",
    "    url = url\n",
    "    string_input= string_input.replace(\" \",\"+\")\n",
    "    result = requests.get(os.path.join(url,\"search?q=\"+string_input))\n",
    "    src = result.content\n",
    "    soup = BeautifulSoup(src,'lxml')\n",
    "    return len(soup.find_all('li',{\"class\":\"inline-block mr-8\"}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p class = heading-32 for full scores\n",
    "def scrape(url = \"https://www.hrc.org/resources/healthcare-facilities/\",string_input=\"New Jersey\",hospital_info = None,sub_criteria_info = None,criteria_info = None):\n",
    "    url = url\n",
    "    string_input= string_input.replace(\" \",\"+\")\n",
    "    for i in range(getPages(url,string_input)):\n",
    "        if(i==0):\n",
    "            result = requests.get(os.path.join(url,\"search?q=\"+string_input))\n",
    "        else:\n",
    "            url2 = url+\"search/\"\n",
    "            print(os.path.join(url,\"p\" + str(i+1) + \"?\" +\"q=\"+string_input))\n",
    "            result = requests.get(os.path.join(url2,\"p\" + str(i+1) + \"?\" +\"q=\"+string_input))\n",
    "            \n",
    "        src = result.content\n",
    "        soup = BeautifulSoup(src,'lxml')\n",
    "        links = soup.find_all('a',{\"class\": \"block text-current\"})\n",
    "        urls = []\n",
    "        title = []\n",
    "        for i in range(len(links)):\n",
    "            arr = np.array(links[i])\n",
    "            if(arr[1][\"class\"][0] == 'align-middle'or arr[1][\"class\"] == 'align-middle'):\n",
    "                begin = re.search('href= ?',links[1].prettify()).start() + 5\n",
    "                end = re.search('\\>(.*)',links[i].prettify()).start()\n",
    "                urls.append(links[i].prettify()[begin:end].replace('\"',''))\n",
    "                title.append(arr[1].get_text())\n",
    "            else:\n",
    "                break;\n",
    "        attributes = []\n",
    "        sub_criteria= []\n",
    "        criteria = []\n",
    "        k = 0\n",
    "        for i in urls:\n",
    "            attributes = []\n",
    "            attributes.append(title[k])\n",
    "            print(title[k])\n",
    "            k+=1\n",
    "            result = requests.get(i)\n",
    "            src = result.content\n",
    "            soup = BeautifulSoup(src,'lxml')\n",
    "            for i in soup.find_all('dd',{\"class\": \"mb-32\"}):\n",
    "                if(i.find_all('a')!= None):\n",
    "                    for j in (i.find_all('a')):\n",
    "                        attributes.append(j['href'])\n",
    "\n",
    "            address = str(np.array(soup.find_all('dd',{\"class\": \"mb-32\"}))[1]).split(\"\\n\")\n",
    "            if(address[1].strip(\" \")[:2]) == '<a':\n",
    "                address = str(np.array(soup.find_all('dd',{\"class\": \"mb-32\"}))).replace(\"\\n\",\"\").replace(\"\\\\n\",\"\\n\").split(\"\\n\")\n",
    "            for i in range(len(address)):\n",
    "                if(address[i]==\"        <br/>\" or address[i] == \"        ' <br/>  '\"):\n",
    "                    address = np.delete(address,i)\n",
    "                    break;\n",
    "            ind = findEndIndex(address)\n",
    "            street_address=\"\"\n",
    "            city = \"\"\n",
    "            state = \"\"\n",
    "            zipCode=\"\"\n",
    "            if(ind == 4):\n",
    "                street_address = address[1].strip(\" \")\n",
    "                city,state = address[2].strip(\" \").split(\",\")\n",
    "                state = state.strip(\" \")\n",
    "                zipCode = address[3].strip(\" \")\n",
    "            if(ind==5):\n",
    "                street_address = address[1].strip(\" \") + \" \" + address[2].strip(\" \")\n",
    "                city,state = address[3].strip(\" \").split(\",\")\n",
    "                state = state.strip(\" \")\n",
    "                zipCode = address[3].strip(\" \")\n",
    "            \n",
    "            attributes.append(street_address)\n",
    "            attributes.append(city)\n",
    "            attributes.append(state)\n",
    "            attributes.append(zipCode)\n",
    "            try:\n",
    "                score = str(soup.find_all('div',{\"class\":\"bg-yellow-500 heading-48 p-16 text-black text-center w-full md:p-32\"})[0].get_text()).split(\"\\n\")[1].strip(\" \")\n",
    "            except:\n",
    "                score = str(soup.find_all('div',{\"class\":\"bg-blue-100 heading-48 p-16 text-black text-center w-full md:p-32\"})[0].get_text()).split(\"\\n\")[1].strip(\" \")\n",
    "                \n",
    "            \n",
    "            attributes.append(score)\n",
    "            sub_criteria = []\n",
    "            criteria = []\n",
    "            for i in soup.find_all('p',{\"class\":\"font-bold text-18 text-right\"}):\n",
    "                sub_criteria.append(str(i.get_text()).strip(\" \").strip(\"\\n\")[24:])\n",
    "            for j in soup.find_all('p',{\"class\":\"heading-32\"}):\n",
    "                criteria.append(str(j.get_text()).split(\"\\n\")[1].strip(\" \"))\n",
    "            hospital_info.append(attributes)\n",
    "            sub_criteria_info.append(sub_criteria)\n",
    "            criteria_info.append(criteria)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hospital_info=[]\n",
    "#sub_criteria_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape(hospital_info = hospital_info,sub_criteria_info = sub_criteria_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
